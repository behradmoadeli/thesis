\section{Model Predictive Controller Design}
In this section, the observer-based MPC shown in Fig.~\ref{fig:block_diagram} is developed with the goal of stabilizing the given unstable infinite-dimensional system within an optimal framework, relying solely on output measurements while satisfying input constraints. 
An infinite-time open-loop objective function sets the foundation of the controller design in the discrete-time setting at each sampling instant $k$. The objective function consists of a weighted sum of actuation costs as well as state deviations, for all future time instances, subject to the system dynamics and input constraints. Since full-state is assumed to be unavailable, reconstructed states are used to estimate states of the system, as shown in \eqref{eq:MPC_inf_time}.
\begin{figure}[!htbp]
    \centering
    \begin{tikzpicture}[node distance=2cm, scale=0.7, transform shape]
        \node (plant) [block, minimum width=3cm] {Plant};
        \node (regulator) [block, below of=plant, xshift=-1cm, yshift=-1cm] {MPC};
        \node (observer) [block, below of=plant, xshift=1cm, yshift=0.5cm] {Observer};
        \draw [arrow] (plant.east) -- node[midway, above] {$y(k)$} ++(2,0);
        \draw [arrow] (plant.east) ++(1,0) |- (observer.east);
        \draw [arrow] (observer.south) -- ++(0,-1) node[midway, right] {$\hat{{x}}(\zeta,k)$} -- (regulator.east);    
        \draw [arrow] (regulator.west) -- ++(-1,0) |- (plant.west);
        \draw [arrow] (regulator.west) ++(-1,1.5) coordinate(start) -- node[near start, left, xshift=-0.75cm] {$u(k)$} (observer.west);
    \end{tikzpicture}
    \caption{Block diagram representation of the observer-based MPC.}
    \label{fig:block_diagram}
\end{figure}

\begin{equation} \label{eq:MPC_inf_time}
    \begin{aligned}
        \min_{U} \quad \sum_{l=0}^{\infty} &\langle \underline{\hat{x}}(\zeta, k+l | k), \mathfrak{Q} \underline{\hat{x}}(\zeta, k+l | k) \rangle \\
        + &\langle u(k+l+1 | k), \mathfrak{F} u(k+l+1|k) \rangle \\
        \, \\
        \text{s.t.} \quad &\underline{\hat{x}}(\zeta, k+l | k) = \mathfrak{A}_d \underline{\hat{x}}(\zeta, k+l-1 | k) + \mathfrak{B}_d u(k+l | k) \\
        &u^{min} \leq u(k+l | k) \leq u^{max}
    \end{aligned}
\end{equation}
where $\mathfrak{Q}$ and $\mathfrak{F}$ are positive definite operators of appropriate dimensions, responsible for penalizing state deviations and actuation costs, respectively. The notation $(k+l|k)$ indicates the future time states or input instance $k+l$ obtained at time $k$. The infinite-time optimization problem may be reduced to a finite-time setup by assigning zero-input beyond a certain control horizon $N$, resulting in the optimization problem in \eqref{eq:MPC_finite_time}.
\begin{equation} \label{eq:MPC_finite_time}
    \begin{aligned}
        \min_{U} \quad \sum_{l=0}^{N-1} &\langle \underline{\hat{x}}(\zeta, k+l | k), \mathfrak{Q} \underline{\hat{x}}(\zeta, k+l | k) \rangle \\
        + &\langle u(k+l+1 | k), \mathfrak{F} u(k+l+1|k) \rangle \\
        + &\langle \underline{\hat{x}}(\zeta, k+N | k), \mathfrak{P} \underline{\hat{x}}(\zeta, k+N | k) \rangle \\
        \, \\
        \text{s.t.} \quad &\underline{\hat{x}}(\zeta, k+l | k) = \mathfrak{A}_d \underline{\hat{x}}(\zeta, k+l-1 | k) + \mathfrak{B}_d u(k+l | k) \\
        &u^{min} \leq u(k+l | k) \leq u^{max} \\
        & \langle \underline{\hat{x}}(\zeta, k+N | k), \underline{\phi_u}(\zeta) \rangle = 0
    \end{aligned}
\end{equation}
where $\mathfrak{P}$ is the terminal cost operator obtained as the solution to the discrete-time Lyapunov equation, as shown in \eqref{eq:terminal_cost}. This operator can be shown to be positive definite only if the terminal state $\underline{\hat{x}}(\zeta, k+N | k)$ is in a stable subspace. Therefore, for the resulting quadratic optimization problem to be convex, an equality constraint is introduced to guarantee $\mathfrak{P}$ is positive definite. The terminal constraint is enforced by setting the projection of the terminal state onto the unstable subspace of the system to zero \cite{curtainbook, xu2017linear, khatibi2021model}.
\begin{equation} \label{eq:terminal_cost}
    \mathfrak{P} (\cdot) = \sum_{m=0}^{\infty} \sum_{n=0}^{\infty} 
    -\frac{
        \langle \underline{\phi_m} , \mathfrak{Q} \underline{\psi_n} \rangle
    }{
        \lambda_m + \overline{\lambda_n}
    }
    \langle (\cdot) , \underline{\psi_n} \rangle \phi_m
\end{equation}

Here, $\underline{\phi_u}(\zeta)$ denotes the set of unstable eigenfunctions of the system, corresponding to all eigenvalues with $\operatorname{Re}(\lambda_u) \geq 0$. In addition, $\underline{\phi_i}$ and $\underline{\psi_i}$ represent the $i^{\text{th}}$ eigenfunctions of the original and adjoint systems, respectively. The resulting finite-horizon control problem can be algebraically manipulated into a format compatible with conventional quadratic programming (QP) solvers, where the input sequence over the prediction horizon $N$ is defined as
\[
U = \begin{bmatrix} u(k+1|k) & u(k+2|k) & \dots & u(k+N|k) \end{bmatrix}^\top.
\]

At each sampling instant $k$, the optimal input sequence $U$ is obtained by solving the QP. However, only the first control input, $u(k+1|k)$, is applied to the system in accordance with the receding horizon strategy. Upon receiving the next output measurement $y(k+1)$, the Luenberger observer reconstructs the current system state at time $k+1$, which then initializes the next optimization cycle. This loop repeats at every time step, enabling real-time output feedback control while reconstructing system states. See Appendix~\ref{app:QP} for mathematical details of the QP formulation.
