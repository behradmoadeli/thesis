\section{Estimation and Control}

\subsection{Model predictive control design, full-state availability}

The proposed full-state feedback model predictive control strategy, as shown in Fig.~\ref{fig:block_diagram}, is developed in this section with the goal of stabilizing the given unstable infinite-dimensional system within an optimal framework while satisfying input constraints. An infinite-time open-loop objective function sets the foundation of the controller design in the discrete-time setting at each sampling instant $k$, which consists of a weighted sum of state deviations and actuation costs for all future time instances, subject to the system dynamics and input constraints, as shown in \eqref{eq:MPC_inf_time}.

\begin{figure}[!htbp]
    \centering
    \begin{tikzpicture}[node distance=2cm, scale=0.75, transform shape]
        \node (plant) [block] {Plant};
        \node (MPC) [block, below of=plant] {MPC};
        \draw [arrow] (plant.south) -- node[midway, right] {$\underline{x}(\zeta,k)$} (MPC.north);
        \draw [arrow] (MPC.west) -- ++(-1,0) |- node[near end, above] {$u(k)$} (plant.west);
        \draw [arrow] (plant.east) -- node[midway, above] {$y(k)$} ++(1,0);
    \end{tikzpicture}
    \caption{Proposed full-state feedback model predictive control system.}
    \label{fig:block_diagram}
\end{figure}

\begin{equation} \label{eq:MPC_inf_time}
    \begin{aligned}
        \min_{U} \quad \sum_{l=0}^{\infty} &\langle \underline{x}(\zeta, k+l | k), \mathfrak{Q} \underline{x}(\zeta, k+l | k) \rangle \\
        + &\langle u(k+l+1 | k), \mathfrak{F} u(k+l+1|k) \rangle \\
        \, \\
        \text{s.t.} \quad &\underline{x}(\zeta, k+l | k) = \mathfrak{A}_d \underline{x}(\zeta, k+l-1 | k) + \mathfrak{B}_d u(k+l | k) \\
        &u^{min} \leq u(k+l | k) \leq u^{max}
    \end{aligned}
\end{equation}

where $\mathfrak{Q}$ and $\mathfrak{F}$ are positive definite operators of appropriate dimensions, responsible for penalizing state deviations and actuation costs, respectively. The notation $(k+l|k)$ indicates the future time states or input instance $k+l$ obtained at time $k$. The infinite-time optimization problem may be reduced to a finite-time setup by assigning zero-input beyond a certain control horizon $N$, resulting in the optimization problem in \eqref{eq:MPC_finite_time}.

\begin{equation} \label{eq:MPC_finite_time}
    \begin{aligned}
        \min_{U} \quad \sum_{l=0}^{N-1} &\langle \underline{x}(\zeta, k+l | k), \mathfrak{Q} \underline{x}(\zeta, k+l | k) \rangle \\
        + &\langle u(k+l+1 | k), \mathfrak{F} u(k+l+1|k) \rangle \\
        + &\langle \underline{x}(\zeta, k+N | k), \mathfrak{P} \underline{x}(\zeta, k+N | k) \rangle \\
        \, \\
        \text{s.t.} \quad &\underline{x}(\zeta, k+l | k) = \mathfrak{A}_d \underline{x}(\zeta, k+l-1 | k) + \mathfrak{B}_d u(k+l | k) \\
        &u^{min} \leq u(k+l | k) \leq u^{max} \\
        & \langle \underline{x}(\zeta, k+N | k), \underline{\phi_u}(\zeta) \rangle = 0
    \end{aligned}
\end{equation}

Obtained as the solution to the discrete-time Lyapunov equation, $\mathfrak{P}$ is the terminal cost operator as shown in \eqref{eq:terminal_cost}; which can be proven to be positive definite only if the terminal state $\underline{x}(\zeta, k+N | k)$ is in a stable subspace. Therefore, an equality constraint is introduced to guarantee that the resulting quadratic optimization problem is convex. The terminal constraint is enforced by setting the projection of the terminal state onto the unstable subspace of the system to zero \cite{curtainbook, xu2017linear, khatibi2021model}. Here, $\underline{\phi_u}(\zeta)$ is the set of unstable eigenfunctions of the system, for all eigenvalues where $\operatorname{Re}(\lambda_u) \geq 0$.

\begin{equation} \label{eq:terminal_cost}
    \mathfrak{P} (\cdot) = \sum_{m=0}^{\infty} \sum_{n=0}^{\infty} 
    -\frac{
        \langle \underline{\phi_m} , \mathfrak{Q} \underline{\psi_n} \rangle
    }{
        \lambda_m + \overline{\lambda_n}
    }
    \langle (\cdot) , \underline{\psi_n} \rangle \phi_m
\end{equation}

One may further process the optimization problem in \eqref{eq:MPC_finite_time} to obtain a standard format for quadratic programming (QP) solvers by substituting the future states in terms of the current state and the sequence of future inputs using system dynamics expression. The resulting QP problem is given in \eqref{eq:MPC_QP}. The optimal input sequence $U$ is then obtained by solving the QP problem at each sampling instant $k$. To implement a receding horizon control strategy, only the first input of the optimal sequence $u(k+1|k)$ is applied to the system, and the optimization problem is solved again at the next sampling instant $k+1$.

\begin{equation} \label{eq:MPC_QP}
    \begin{aligned}
        \min_{U} &J = U^T \langle I,H \rangle U + 2U^T \langle I, P \underline{x}(\zeta, k|k) \rangle \\
        \text{s.t.} &\qquad U^{min} \leq U \leq U^{max} \\
        &\qquad T_u \underline{x}(\zeta, k|k) + S_u U = 0
        \, \\
        \text{with } &H = \\
        &\hspace{-3.5em }\begin{bmatrix}
            \mathfrak{B}_d^* \mathfrak{P} \mathfrak{B}_d + \mathfrak{F} & \mathfrak{B}_d^* \mathfrak{A}_d^* \mathfrak{P} \mathfrak{B}_d & \cdots &  \mathfrak{B}_d^* {\mathfrak{A}_d^*}^{N-1} \mathfrak{P} \mathfrak{B}_d \\
            \mathfrak{B}_d^* \mathfrak{P} \mathfrak{A}_d \mathfrak{B}_d & \mathfrak{B}_d^* \mathfrak{P} \mathfrak{B}_d + \mathfrak{F} & \cdots & \mathfrak{B}_d^* {\mathfrak{A}_d^*}^{N-2} \mathfrak{P} \mathfrak{B}_d \\
            \vdots & \vdots & \ddots & \vdots \\
            \mathfrak{B}_d^* \mathfrak{P} {\mathfrak{A}_d}^{N-1} \mathfrak{B}_d & \mathfrak{B}_d^* \mathfrak{P} {\mathfrak{A}_d}^{N-2} \mathfrak{B}_d & \cdots & \mathfrak{B}_d^* \mathfrak{P} \mathfrak{B}_d + \mathfrak{F}
        \end{bmatrix} \\
        P = &\begin{bmatrix}
            \mathfrak{B}_d^* \mathfrak{P} {\mathfrak{A}_d} &
            \mathfrak{B}_d^* \mathfrak{P} {\mathfrak{A}_d}^{2}  &
            \hdots &
            \mathfrak{B}_d^* \mathfrak{P} {\mathfrak{A}_d}^{N} 
        \end{bmatrix}^T \\
        T_u (\cdot) = &\begin{bmatrix}
            \langle {\mathfrak{A}_d}^{N} (\cdot), \underline{\phi_u} \rangle
        \end{bmatrix} \\
        S_u = &\begin{bmatrix}
            \langle {\mathfrak{A}_d}^{N-1} \mathfrak{B}_d, \underline{\phi_u} \rangle & 
            \langle {\mathfrak{A}_d}^{N-2} \mathfrak{B}_d, \underline{\phi_u} \rangle &
            \hdots &
            \langle \mathfrak{B}_d, \underline{\phi_u} \rangle
        \end{bmatrix} \\
        U = &\begin{bmatrix}
            u(k+1|k) & u(k+2|k) & \hdots & u(k+N|k)
        \end{bmatrix}^T
    \end{aligned}
\end{equation}


\subsection{Continuous-Time Observer Design}
For the purpose of state reconstruction of a diffusion-convection-reaction system, where the feedforward term $\mathfrak{D}$ is generally absent, the continuous-time observer dynamics are given by \eqref{eq:observer_continuous}.
\begin{equation} \label{eq:observer_continuous}
    \begin{aligned}
        \dot{\underline{\hat{x}}}(\zeta, t) &= \mathfrak{A} \underline{\hat{x}}(\zeta, t) + \mathfrak{B} u(t) + \mathfrak{L}_c [y(t) - \hat{y}(t)] \\
        \hat{y}(t) &= \mathfrak{C} \underline{\hat{x}}(\zeta, t)
    \end{aligned}
\end{equation}
where $\underline{\hat{x}}(\zeta, t)$ is the reconstructed state of the original system and $\mathfrak{L}_c$ is the continuous-time observer gain. By subtracting the observer dynamics from the original system dynamics, the error dynamics $e(\zeta,t$) are obtained as shown in \eqref{eq:observer_error_continuous}.
\begin{equation} \label{eq:observer_error_continuous}
    \begin{aligned}
        \dot{e}(\zeta, t) &= (\mathfrak{A} - \mathfrak{L}_c \mathfrak{C}) e(\zeta, t) \equiv \mathfrak{A}_o e(\zeta,t) \\
    \end{aligned}
\end{equation}
The goal is to design the observer gain $\mathfrak{L}_c$ such that the error dynamics are exponentially stable, i.e. $\max\{\operatorname{Re}(\lambda_{o})\}~<~0$ where $\{\lambda_{o}\}$ is the set of eigenvalues of the error dynamics operator $\mathfrak{A}_o$. Three different forms of the observer gain are considered as spatial functions $\mathfrak{L}_c = f(\zeta, l_{obs})$ with the effect of the scalar coefficient $l_{obs}$ on $\max\{\operatorname{Re}(\lambda_{o})\}$ shown in Fig.~\ref{fig:L_vs_lambda}.

\begin{figure}[!htbp]
    \centering
    \includesvg[inkscapelatex=false, height=0.23\textwidth, keepaspectratio]{figures/obs_lambda.svg}
    \caption{The effect of various observer gains $\mathfrak{L}_c = f(\zeta, l_{obs})$ on the eigenvalues of state reconstruction error dynamics $\lambda_o$.}
    \label{fig:L_vs_lambda}
\end{figure}

\subsection{Discrete-Time Observer Design}
Once an appropriate continuous-time observer gain is determined, the discrete-time observer gain $\mathfrak{L}_d$ may be obtained using the same Cayley-Tustin time discretization approach, as shown in \eqref{eq:observer_discrete}.
\begin{equation} \label{eq:observer_discrete}
    \begin{aligned}
        \underline{\hat{x}}(\zeta, k) &= \mathfrak{A}_d \underline{\hat{x}}(\zeta, k-1) + \mathfrak{B}_d u(k) + \mathfrak{L}_d [y(k) - \hat{y}(k)] \\
        \hat{y}(k) &= \mathfrak{C}_{d,o} \underline{\hat{x}}(\zeta, k-1) + \mathfrak{D}_{d,o} u(k) + \mathfrak{M}_{d,o} y(k)
    \end{aligned}
\end{equation}
with $\mathfrak{A}_d$ and $\mathfrak{B}_d$ defined in \eqref{eq:discrete_AB}, and $\mathfrak{C}_{d,o}$, $\mathfrak{D}_{d,o}$, $\mathfrak{M}_{d,o}$, and $\mathfrak{L}_d$ are given in \eqref{eq:observer_discrete_CDLM}.
\begin{equation} \label{eq:observer_discrete_CDLM}
    \begin{aligned}
        \mathfrak{C}_{d,o} (\cdot) &= \sqrt{2\alpha} \left[ I + \mathfrak{C} (\alpha I - \mathfrak{A}) \mathfrak{L}_c \right]^{-1} \mathfrak{C} \mathfrak{R}(\alpha, \mathfrak{A}) (\cdot) \\
        \mathfrak{D}_{d,o} &= \left[ I + \mathfrak{C} (\alpha I - \mathfrak{A}) \mathfrak{L}_c \right]^{-1} \mathfrak{C} \mathfrak{R}(\alpha, \mathfrak{A}) \mathfrak{B} \\
        \mathfrak{M}_{d,o} &= \left[ I + \mathfrak{C} \mathfrak{R}(\alpha, \mathfrak{A}) \mathfrak{L}_c \right]^{-1} \mathfrak{C} \mathfrak{R}(\alpha, \mathfrak{A}) \mathfrak{L}_c \\
        \mathfrak{L}_d &= \sqrt{2\alpha} \mathfrak{R}(\alpha, \mathfrak{A}) \mathfrak{L}_c \\
    \end{aligned}
\end{equation}
It can been shown that using this approach, the discrete-time error dynamics will be stable if the continuous-time observer gain $\mathfrak{L}_c$ is chosen such that $\mathfrak{A}_o$ is stable. It is also worth noting that the proposed methodology skips the need for model reduction associated with the discrete-time Luenberger observer, with no spatial approximation required as well \cite{dochain2000state,dochain2001state,alonso2004optimal,ali2015review,khatibi2021model}.

\subsection{Model predictive control design, output feedback implementation}

To enable real-time implementation under limited state access, the discrete-time model predictive controller is now augmented with the obtained discrete-time Luenberger observer. The reconstructed state $\underline{\hat{x}}(\zeta,k)$ is substituted for the full state in the MPC formulation, yielding an observer-based output-feedback controller, as illustrated in Fig.~\ref{fig:block_diagram_obs}.

\begin{figure}[!htbp]
    \centering
    \begin{tikzpicture}[node distance=2cm, scale=0.7, transform shape]
        \node (plant) [block, minimum width=3cm] {Plant};
        \node (regulator) [block, below of=plant, xshift=-1cm, yshift=-1cm] {MPC};
        \node (observer) [block, below of=plant, xshift=1cm, yshift=0.5cm] {Observer};
        \draw [arrow] (plant.east) -- node[midway, above] {$y(k)$} ++(2,0);
        \draw [arrow] (plant.east) ++(1,0) |- (observer.east);
        \draw [arrow] (observer.south) -- ++(0,-1) node[midway, right] {$\underline{\hat{x}}(\zeta,k)$} -- (regulator.east);    
        \draw [arrow] (regulator.west) -- ++(-1,0) |- (plant.west);
        \draw [arrow] (regulator.west) ++(-1,1.5) coordinate(start) -- node[near start, left, xshift=-0.75cm] {$u(k)$} (observer.west);
    \end{tikzpicture}
    \caption{Block diagram representation of the observer-based MPC.}
    \label{fig:block_diagram_obs}
\end{figure}

The cost function and terminal condition remain unchanged, but the predicted state trajectory is now driven by the estimated state:

\begin{equation} \label{eq:MPC_finite_time_hat}
    \begin{aligned}
        \min_{U} \quad \sum_{l=0}^{N-1} &\langle \underline{\hat{x}}(\zeta, k+l | k), \mathfrak{Q} \underline{\hat{x}}(\zeta, k+l | k) \rangle \\
        + &\langle u(k+l+1 | k), \mathfrak{F} u(k+l+1|k) \rangle \\
        + &\langle \underline{\hat{x}}(\zeta, k+N | k), \mathfrak{P} \underline{\hat{x}}(\zeta, k+N | k) \rangle \\
        \, \\
        \text{s.t.} \quad &\underline{\hat{x}}(\zeta, k+l | k) = \mathfrak{A}_d \underline{\hat{x}}(\zeta, k+l-1 | k) + \mathfrak{B}_d u(k+l | k) \\
        &u^{min} \leq u(k+l | k) \leq u^{max} \\
        & \langle \underline{\hat{x}}(\zeta, k+N | k), \underline{\phi_u}(\zeta) \rangle = 0
    \end{aligned}
\end{equation}

The observer provides $\underline{\hat{x}}(\zeta,k)$ at each time step by processing most recent output and control input. This reconstructed state initializes the prediction horizon and closes the loop in the absence of full-state access. The resulting control law inherits all properties of the full-state MPC while enabling output feedback implementation. The QP formulation follows analogously by substituting $\underline{x}$ with $\underline{\hat{x}}$ in \eqref{eq:MPC_QP}, and is detailed in \eqref{eq:MPC_QP_hat}.

\begin{equation} \label{eq:MPC_QP_hat}
    \begin{aligned}
        \min_{U} &J = U^\top \langle I,H \rangle U + 2U^\top \langle I, P \underline{\hat{x}}(\zeta, k|k) \rangle \\
        \text{s.t.} &\qquad U^{min} \leq U \leq U^{max} \\
        &\qquad T_u \underline{\hat{x}}(\zeta, k|k) + S_u U = 0
        \, \\
        \text{with } &H = \\
        &\hspace{-3.5em }\begin{bmatrix}
            \mathfrak{B}_d^* \mathfrak{P} \mathfrak{B}_d + \mathfrak{F} & \mathfrak{B}_d^* \mathfrak{A}_d^* \mathfrak{P} \mathfrak{B}_d & \cdots &  \mathfrak{B}_d^* {\mathfrak{A}_d^*}^{N-1} \mathfrak{P} \mathfrak{B}_d \\
            \mathfrak{B}_d^* \mathfrak{P} \mathfrak{A}_d \mathfrak{B}_d & \mathfrak{B}_d^* \mathfrak{P} \mathfrak{B}_d + \mathfrak{F} & \cdots & \mathfrak{B}_d^* {\mathfrak{A}_d^*}^{N-2} \mathfrak{P} \mathfrak{B}_d \\
            \vdots & \vdots & \ddots & \vdots \\
            \mathfrak{B}_d^* \mathfrak{P} {\mathfrak{A}_d}^{N-1} \mathfrak{B}_d & \mathfrak{B}_d^* \mathfrak{P} {\mathfrak{A}_d}^{N-2} \mathfrak{B}_d & \cdots & \mathfrak{B}_d^* \mathfrak{P} \mathfrak{B}_d + \mathfrak{F}
        \end{bmatrix} \\
        P = &\begin{bmatrix}
            \mathfrak{B}_d^* \mathfrak{P} {\mathfrak{A}_d} &
            \mathfrak{B}_d^* \mathfrak{P} {\mathfrak{A}_d}^{2}  &
            \hdots &
            \mathfrak{B}_d^* \mathfrak{P} {\mathfrak{A}_d}^{N} 
        \end{bmatrix}^\top \\
        T_u (\cdot) = &\begin{bmatrix}
            \langle {\mathfrak{A}_d}^{N} (\cdot), \underline{\phi_u} \rangle
        \end{bmatrix} \\
        S_u = &\begin{bmatrix}
            \langle {\mathfrak{A}_d}^{N-1} \mathfrak{B}_d, \underline{\phi_u} \rangle & 
            \langle {\mathfrak{A}_d}^{N-2} \mathfrak{B}_d, \underline{\phi_u} \rangle &
            \hdots &
            \langle \mathfrak{B}_d, \underline{\phi_u} \rangle
        \end{bmatrix} \\
        U = &\begin{bmatrix}
            u(k+1|k) & u(k+2|k) & \hdots & u(k+N|k)
        \end{bmatrix}^\top
    \end{aligned}
\end{equation}